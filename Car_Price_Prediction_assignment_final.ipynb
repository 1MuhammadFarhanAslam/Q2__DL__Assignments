{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Car_Price_Prediction_assignment_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_HsWVrWvDNP"
      },
      "source": [
        "# Car Price Prediction::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neq61yUOvDNV"
      },
      "source": [
        "Download dataset from this link:\n",
        "\n",
        "https://www.kaggle.com/hellbuoy/car-price-prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "errs9eyZvDNW"
      },
      "source": [
        "# Problem Statement::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92PLZp2kwKxv"
      },
      "source": [
        "# mount google drive in to your Colab enviornment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvDEDSF3yS1S"
      },
      "source": [
        "\"/content/CarPrice_Assignment.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCu6NRLVvDNX"
      },
      "source": [
        "A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n",
        "\n",
        "They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n",
        "\n",
        "Which variables are significant in predicting the price of a car\n",
        "How well those variables describe the price of a car\n",
        "Based on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n",
        "\n",
        "# task::\n",
        "We are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vI6CymvDNX"
      },
      "source": [
        "# WORKFLOW ::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRvlK8RBvDNX"
      },
      "source": [
        "1.Load Data\n",
        "\n",
        "2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n",
        "\n",
        "3.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n",
        "\n",
        "4.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n",
        "\n",
        "5.Compilation Step (Note : Its a Regression problem , select loss , metrics according to it)\n",
        "6.Train the Model with Epochs (100) and validate it\n",
        "\n",
        "7.If the model gets overfit tune your model by changing the units , No. of layers , activation function , epochs , add dropout layer or add Regularizer according to the need .\n",
        "\n",
        "8.Evaluation Step\n",
        "\n",
        "9.Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duXMXXhZvDNY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "car_data = pd.read_csv('/content/CarPrice_Assignment.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd2sfUEUPRvi"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnf-BIvL5-qO"
      },
      "source": [
        "car_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JRgNW0hzOVH"
      },
      "source": [
        "car_data['CarName'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NTeH-qvDNa"
      },
      "source": [
        "#check if there are empty cells, if there are then row and column indexes will be returned where values are empty or missing\n",
        "np.where(car_data.applymap(lambda x: x ==''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCqFI3Y0l6h"
      },
      "source": [
        "car_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwZgj7IIvXRm"
      },
      "source": [
        "# correct the name error in audi 100 ls\n",
        "car_data.iloc[3,2] = 'audi 100ls'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1cCvWApvDNZ"
      },
      "source": [
        "car_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCSHog6LFfdp"
      },
      "source": [
        "car_data.drop(columns=['car_ID'], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6vZOZr5EpP"
      },
      "source": [
        "# get columns so that we can use the column names for onehot encoding of catagorical featrues in next cell\n",
        "car_data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqvKX-JyvDNa"
      },
      "source": [
        "# onehot encode all catagorical columns\n",
        "final_car = pd.get_dummies(car_data, columns=['CarName','symboling','fueltype',\t'aspiration',\t'doornumber',\t'carbody',\t'drivewheel',\t'enginelocation',\t'enginetype',\t'cylindernumber',\t'fuelsystem'], drop_first = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUW7m4p1C_Iz"
      },
      "source": [
        "final_car.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgIZc8J6UswW"
      },
      "source": [
        "#check statistical data to see abnormal values and outliers\n",
        "final_car.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_sikuUQAGD4"
      },
      "source": [
        "#initialize a seed value so that each time we can get the same random number sequence, it will help us  as a team\n",
        "# working on a common project to work on the same random data. Each new seed will generate a particular sequnce\n",
        "#of random number. You can choose any seed value here of your choice\n",
        "# 0.72 means we have taken 72% values for training set as we will make 72/4 = 18 rows of k fold validation data, where\n",
        "# value of k will be 4 when we compile and fit our model for validation\n",
        "np.random.seed(11111)\n",
        "msk = np.random.rand(len(final_car)) < 0.72\n",
        "train_total = final_car[msk]\n",
        "test_total = final_car[~msk]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSjX5UckTu1_"
      },
      "source": [
        "#check the length of our test and train datasets\n",
        "print(len(train_total))\n",
        "print(len(test_total))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4njPsfOfLJP"
      },
      "source": [
        "train_total.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeYOhn1Eiams"
      },
      "source": [
        "# check statistical overview if there are some outliers and abnormal values\n",
        "train_total.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwv0k5MXkjiU"
      },
      "source": [
        "print(train_total.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMVbWuxTzEUh"
      },
      "source": [
        "# get our price labels and store in another dataframe\n",
        "train_label = train_total.loc[:,'price']\n",
        "test_label = test_total.loc[:,'price']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok93gWC0V7RC"
      },
      "source": [
        "train_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pj0hGliz6ld"
      },
      "source": [
        "# drop price from oroginal training and test dataset , as price is not needed there\n",
        "test_data= test_total.drop(columns = ['price'])\n",
        "train_data= train_total.drop(columns = ['price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uw8yz0e1RjS"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic0-eZgDZjsV"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXSdXZs5InD"
      },
      "source": [
        "#get indices of the columns so that we can know how many columns we have to normalize, as catagorical columns which we\n",
        "# have added with onehot encoding, do not need to be normalized.. normalizing will be done in next cell\n",
        "{train_data.columns.get_loc(c): c for idx, c in enumerate(train_data.columns)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTvnqjLaFSYr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5vhggTyRyw"
      },
      "source": [
        "## we normalize data because data has big vlaues in decimal and it will worsen performance of our model, may overfit \n",
        "## or  we may face hardware resource high usage\n",
        "# we will apply the formula normalized_train_data = (train_data - mean)/ stadrad_deviation\n",
        "## firt take mean of training, then subtract mean from each value of the array slice train_data.iloc[:,0:13]\n",
        "mean = train_data.iloc[:,0:13].mean(axis=0) # taking the mean of \n",
        "train_data.iloc[:,0:13] -= mean\n",
        "std = train_data.iloc[:,0:13].std(axis=0)\n",
        "train_data.iloc[:,0:13] /= std\n",
        "test_data.iloc[:,0:13] -= mean\n",
        "test_data.iloc[:,0:13] /= std\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdXge3xhAyiN"
      },
      "source": [
        "mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dKIOrdXIEDr"
      },
      "source": [
        "std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZYfOs0cfaGE"
      },
      "source": [
        "mean_label = train_label.mean()\n",
        "train_label -= mean_label\n",
        "std_label = train_label.std()\n",
        "train_label /= std_label\n",
        "test_label -= mean_label\n",
        "test_label /= std_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UwH-6fJG4RB"
      },
      "source": [
        "mean_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0HkwMimH-9W"
      },
      "source": [
        "std_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mc7B1amLmsx"
      },
      "source": [
        "print(mean_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8uIRUHEw7g4"
      },
      "source": [
        "test_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwYKROqCyWwi"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejyRo91P9Klh"
      },
      "source": [
        "#store in numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLTpt0qn-R5y"
      },
      "source": [
        "test = np.array(test_data.iloc[:]).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ym52Tun-vhn"
      },
      "source": [
        "train = np.array(train_data.iloc[:]).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnFGfxGn_d1b"
      },
      "source": [
        "test_l= np.array(test_label.astype('float32'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kklQZ0za_qZb"
      },
      "source": [
        "train_l= np.array(train_label.astype('float32'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVbXDcGe5cY2"
      },
      "source": [
        "train.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLkQAzx9W0f"
      },
      "source": [
        "(141,192)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS0mDY0d9y3Q"
      },
      "source": [
        "train.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm416iVz953_"
      },
      "source": [
        "\n",
        "# Models section\n",
        "```\n",
        "#WE will configure different models here according to relu, tanh , regularization, dropout etc..\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqOJjh8n8fMU"
      },
      "source": [
        "# we are passing activation function as a parameter here so that we can call this function with tanh or relu while\n",
        "# fitting and training the model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model(act):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(128, activation= act,input_shape=(train.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation= act))\n",
        "  model.add(layers.Dense(32, activation= act))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKs2Axd19Rsx"
      },
      "source": [
        "build_model('relu').summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShTLMMUPWE2E"
      },
      "source": [
        "build_model('tanh').summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLNbzXjKqGSw"
      },
      "source": [
        "# Regularized model\n",
        "from keras import regularizers\n",
        "def build_model_regular(act):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(10, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001),input_shape=(train.shape[1],)))\n",
        "  model.add(layers.Dense(8, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "  model.add(layers.Dense(6, activation= act,kernel_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afnNh94NrncJ"
      },
      "source": [
        "build_model_regular('tanh').summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKOI8TLD1SHd"
      },
      "source": [
        "# dropout model\n",
        "from keras import regularizers\n",
        "def build_model_drop(act):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(10, activation= act,input_shape=(train.shape[1],)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(8, activation= act))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(6, activation= act))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_3icFwvKuGy"
      },
      "source": [
        "build_model_drop('relu').summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwAm7RMr_eMa"
      },
      "source": [
        "# K Fold validation section\n",
        "## here we will use len(train)//k to make 141//4 = 36 rows for validation in each validation test and collect the validation scores for relu , tanh , regularization , and dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTizsf6znpb7"
      },
      "source": [
        "#k fold validation with relu\n",
        "# 141/4\n",
        "import numpy as np\n",
        "k =  4\n",
        "num_val_samples = len(train) // k\n",
        "num_epochs = 100\n",
        "all_scores_relu = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_l[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate([train[:i * num_val_samples],train[(i + 1) * num_val_samples:]],  axis=0)\n",
        "  # print(partial_train_data)\n",
        "  partial_train_targets = np.concatenate([train_l[:i * num_val_samples],train_l[(i + 1) * num_val_samples:]],axis=0)\n",
        "  model = build_model('relu')\n",
        "  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores_relu.append(val_mae)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaXgBohaz4WI"
      },
      "source": [
        "# 141/4\n",
        "#k fold validation with tanh\n",
        "import numpy as np\n",
        "k =  4\n",
        "num_val_samples = len(train) // k\n",
        "num_epochs = 100\n",
        "all_scores_tanh = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_l[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate([train[:i * num_val_samples],train[(i + 1) * num_val_samples:]],  axis=0)\n",
        "  # print(partial_train_data)\n",
        "  partial_train_targets = np.concatenate([train_l[:i * num_val_samples],train_l[(i + 1) * num_val_samples:]],axis=0)\n",
        "  model = build_model('tanh')\n",
        "  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores_tanh.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6TuagJb54U"
      },
      "source": [
        "#k-fold validtion with regularization\n",
        "import numpy as np\n",
        "k =  4\n",
        "num_val_samples = len(train) // k\n",
        "num_epochs = 100\n",
        "all_scores_regular = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_l[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate([train[:i * num_val_samples],train[(i + 1) * num_val_samples:]],  axis=0)\n",
        "  # print(partial_train_data)\n",
        "  partial_train_targets = np.concatenate([train_l[:i * num_val_samples],train_l[(i + 1) * num_val_samples:]],axis=0)\n",
        "  model = build_model_regular('relu')\n",
        "  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores_regular.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ENBDAMrHtb"
      },
      "source": [
        "#k-fold validtion with dropout\n",
        "import numpy as np\n",
        "k =  4\n",
        "num_val_samples = len(train) // k\n",
        "num_epochs = 100\n",
        "all_scores_drop = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_l[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate([train[:i * num_val_samples],train[(i + 1) * num_val_samples:]],  axis=0)\n",
        "  # print(partial_train_data)\n",
        "  partial_train_targets = np.concatenate([train_l[:i * num_val_samples],train_l[(i + 1) * num_val_samples:]],axis=0)\n",
        "  model = build_model_drop('relu')\n",
        "  model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=1)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
        "  all_scores_drop.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSlCIMtT_Gfd"
      },
      "source": [
        "# Scores\n",
        "## here we will see  MAE mean absolute Error scores of all model which we have saved in the list during each training in above section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We_C7ivQsh8Y"
      },
      "source": [
        "all_scores_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1gBTB5DrDA9"
      },
      "source": [
        "all_scores_tanh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf7O0WnN9bVF"
      },
      "source": [
        "all_scores_regular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8LxbiI0cpx"
      },
      "source": [
        "all_scores_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxMnoqoAYd-"
      },
      "source": [
        "# training on the training data\n",
        "## here we will call each model separately from Models section and train on the training data and evaluate on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VORrpehRb7UR"
      },
      "source": [
        "\n",
        "model_tanh = build_model('tanh')\n",
        "model_tanh.fit(train, train_l,epochs= 100, batch_size=1, verbose=0)\n",
        "test_mse_score, test_mae_score = model_tanh.evaluate(test, test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttydi9BzUnf5"
      },
      "source": [
        "model_relu = build_model('relu')\n",
        "model_relu.fit(train, train_l,epochs= 100, batch_size=1, verbose=0)\n",
        "test_mse_score, test_mae_score = model_relu.evaluate(test, test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZFYcfHftNlY"
      },
      "source": [
        "model_regular = build_model_regular('relu')\n",
        "model_regular.fit(train, train_l,epochs= 100, batch_size=1, verbose=0)\n",
        "test_mse_score, test_mae_score = model_regular.evaluate(test, test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSZLSCb0g7c"
      },
      "source": [
        "model_drop = build_model_drop('relu')\n",
        "model_drop.fit(train, train_l,epochs= 100, batch_size=1, verbose=0)\n",
        "test_mse_score, test_mae_score = model_drop.evaluate(test, test_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV42POJyA-Na"
      },
      "source": [
        "# Prediction Section\n",
        "## here we will predict our prices of our test dataset with each model which we have trained in training section\n",
        "## Note that here we will use the reverse process of Normalization to retrieve our values of price in thousand of dollars i.e. x = (y - mean)/ std ==>> we will calculate( y = x * std + mean) and then we will compare it with our target values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5QU9eNHg5_b"
      },
      "source": [
        "test_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mua_UYVJ7YNd"
      },
      "source": [
        "def predict(model, m):\n",
        "  print(f\" the Actual value Price was : {test_l[m]* std_label + mean_label} \" )\n",
        "  return(f\" the predicted Price was : {(model.predict(test[m:m+1].reshape(1,test.shape[1]))) * std_label + mean_label} \")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofX9VqMVbhBC"
      },
      "source": [
        "x_tanh = predict(model_tanh,2)\n",
        "x_tanh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z0UyL_LOImw"
      },
      "source": [
        "x_relu = predict(model_relu,2)\n",
        "x_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlQRNOo0tb8J"
      },
      "source": [
        "x_regular = predict(model_regular,2)\n",
        "x_regular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW5Y65pP0qET"
      },
      "source": [
        "x_drop = predict(model_drop,2)\n",
        "x_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYpye06_xzjc"
      },
      "source": [
        "def plot_fn(mod):\n",
        "  y_true = test_l* std_label + mean_label\n",
        "  y_pred = mod.predict(test) * std_label + mean_label\n",
        "  return y_true , y_pred.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leNMnlxfx5nt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def plotting(mod, label):\n",
        "  y_true, y_pred = plot_fn(mod)\n",
        "  coef = np.polyfit(y_true,y_pred,1)\n",
        "  poly1d_fn = np.poly1d(coef) \n",
        "  # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
        "  plt.figure()\n",
        "  plt.plot(y_true,y_pred, 'yo', y_true, poly1d_fn(y_true), '--k')\n",
        "  plt.title(label)\n",
        "  plt.xlabel('Thousand Dollar True' )\n",
        "  plt.ylabel('Thousand Dollar Predictions' )\n",
        "  plt.xlim(0, 50000)\n",
        "  plt.ylim(0, 50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rQG1a79x72V"
      },
      "source": [
        "plot_list = []\n",
        "for i,j in enumerate([model_relu, model_tanh, model_regular, model_drop]):\n",
        "  list_name = ['model_relu', 'model_tanh', 'model_regular', 'model_drop']\n",
        "  plot_list.append(plotting(j,list_name[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}